from langchain_community.document_loaders import PyPDFLoader
import requests
from pathlib import Path
import requests


#find a online pdf with text that you want to analyze. I'm using Romeo & Juliet for example
url = "https://folger-main-site-assets.s3.amazonaws.com/uploads/2022/11/romeo-and-juliet_PDF_FolgerShakespeare.pdf"
#give the file a name
pdf_name = "romeo_and_juliet.pdf"
filename = Path(pdf_name)
response = requests.get(url)
filename.write_bytes(response.content)


# Load and read PDF
def load_pdf_content(pdf_path):
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    return " ".join([doc.page_content for doc in documents])


# Use Ollama to query with PDF context. Make sure to use ollama pull to get the model first. Here I'm using Google's gemma3 model with 4 billion parameters
# using terminal, after installing ollama the following code will get you this model: ollama pull gemma3:4b

def query_with_cag(context, query):
    prompt = f"Context:\n{context}\n\nQuery: {query}\nAnswer:"
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "gemma3:4b",
            "prompt": prompt,
            "temperature": 0.2,
            "stream": False
        }
    )
    return response.json()["response"].strip()


# Load knowledge base from PDF
knowledge_base = load_pdf_content(pdf_name)


# let's start asking the AI questions, type exit to stop
while True:
    print()
    user_question = input("What would you like to ask? (type 'exit' to quit): ").strip()
    
    if user_question.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break

    response = query_with_cag(knowledge_base, user_question)
    print("Answer:", response)




