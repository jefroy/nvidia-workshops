import ollama
from langchain_community.document_loaders import PyPDFDirectoryLoader

# Step 1: Load and cache knowledge base from PDF directory
def load_knowledge_base(directory_path):
    loader = PyPDFDirectoryLoader(directory_path)
    documents = loader.load()
    return " ".join([doc.page_content for doc in documents])

# Ask user which model to use
model_name = input("Enter the Ollama model you want to use (e.g., 'gemma3:27b', 'mistral', etc.): ").strip()

# Ask user which directory has the pdfs to intake
pdf_location = input("Which directory should be used?: ").strip()

# Load once and keep in memory
pdf_directory = pdf_location  # Change to your directory path
knowledge_base = load_knowledge_base(pdf_directory)

# Step 2: Start question loop with cached context
print(f"\nUsing model: {model_name}")
print("Knowledge base loaded. You can now ask questions.")
print("Type 'exit' to quit.\n")

while True:
    user_question = input("Enter your question: ").strip()
    if user_question.lower() in ["exit", "quit"]:
        print("Goodbye!")
        break

    prompt = f"""

Context:{knowledge_base}

Question: {user_question}
"""

    response = ollama.chat(
    model=model_name,
    messages=[
        {
            "role": "system",
            "content": (
                "You are a detail-oriented CIA intelligence officer who writes reports on the Shadowrun's 6th World setting. You respond to queries with thorough reports that provide answers and also cover strengths, weaknesses, opportunities, and threats (SWOT analysis) where appropriate. End the report with an excerpt of an interview with a veteran shadowrunner for their perspective. If you don't know the answer, just say that you don't know."
            )
        },
        {
            "role": "user",
            "content": f"""
Context:
{knowledge_base}

Question: {user_question}
"""
        }
    ]
)


    print("\nAnswer:", response['message']['content'], "\n")




